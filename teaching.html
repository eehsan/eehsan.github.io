<!DOCTYPE html>
<html lang="en"><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">

	<meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <meta name="description" content="Ehsan Shareghi's homepage">
    <meta name="author" content="Ehsan Shareghi">
    <meta name="keywords" content="ehsan, shareghi, homepage, phd, cv, curriculum vitae, resume, research, projects, publications, nlp, natural language processing, nonparametric bayesian, machine learning, compressed data structures, data science, bayesian inference, deep learning, university of cambridge, monash university">

    <!-- <link rel="icon" href="favicon.ico"> uncomment to include a favicon; I don't have one yet -->
    <link rel="stylesheet" type="text/css" href="styles/bootstrap.css">
    <!--link rel="stylesheet" type="text/css" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/css/bootstrap-theme.min.css"-->
    <link rel="stylesheet" type="text/css" href="styles/ie10-viewport-bug-workaround.css">
    <link rel="stylesheet" type="text/css" href="styles/font-awesome.css">
    <link rel="stylesheet" type="text/css" href="styles/bootstrap-social.css">
    <link rel="stylesheet" type="text/css" href="styles/index.css">

    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
    	<script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
      	<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->

	<title>Ehsan Shareghi</title>
	<!-- base href="https://eehsan.github.io/" -->
<style>
.p1 {
  font-family: "Papyrus";
}
</style>
</head>

<!--body onload="setMyEmAddr();"-->
<body>

	<nav class="navbar navbar-inverse navbar-fixed-top">
      <div class="container">
        <div class="navbar-header">
          <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <a class="navbar-brand" target="_self" href="https://eehsan.github.io/">Ehsan Shareghi</a>
        </div>
        <div id="navbar" class="collapse navbar-collapse">
          <ul class="nav navbar-nav">
            <li><a target="_self" href="publication.html">Publications</a></li>
	          <li><a target="_self" href="people.html">People</a></li>
            <li><a target="_self" href="teaching.html">Teaching</a></li>
            <li><a target="_self" href="mailto:ehsan.shareghi@monash.edu">Contact</a></li>
            <!--li><a target="_self" href="#contact">Contact</a></li-->
          </ul>
        </div><!--/.nav-collapse -->
      </div>
    </nav>

    <div class="container">
	    <p>
		<strong><a class="quiet p1" href="https://www.monash.edu">Monash University</a>.</strong>
		<br>
		<p><span class="text-muted"><span class="text-muted"> <em>CE/Lecturer, Natural Language Processing (FIT5217, 2022-2025). Syllabus: </em><br /></span></span></p>
<ul>
	<li><strong><span style="font-family:tahoma,sans-serif">Week 1 Introduction to Natural Language Processing</span></strong></li>
	<li><span style="font-family:tahoma,sans-serif"><strong>Week 2 Language Modelling</strong></span>
	<ul>
		<li>What is a language model?</li>
		<li>Context Length</li>
		<li>The Chain Rule of probability</li>
		<li>n-gram language models</li>
		<li>Data sparsity issues</li>
		<li>Smoothed n-grams
		<ul>
			<li>Add-k</li>
			<li>Kneser-Ney (not examinable)</li>
			<li>Stupid Backoff</li>
		</ul>
		</li>
		<li>Evaluating model performance</li>
	</ul>
	</li>
	<li><span style="font-family:tahoma,sans-serif"><strong>Week 3 Sequence Labelling</strong></span>
	<ul>
		<li>Word categories</li>
		<li>Part-of-Speech (POS) Tagging</li>
		<li>Other Sequence Labelling Problems</li>
		<li>Hidden Markov Model (HMM)</li>
		<li>Observation Likelihood</li>
		<li>Most Likely State Sequence</li>
		<li>Supervised Learning of HMM</li>
		<li>Evaluation</li>
	</ul>
	</li>
	<li><span style="font-family:tahoma,sans-serif"><strong>Week 4 Syntactic Parsing</strong></span>
	<ul>
		<li>Syntax</li>
		<li>Syntactic Parsing</li>
		<li>CKY Parsing</li>
		<li>Limitations of Context Free Grammars
		<ul>
			<li>Statistical Parsing</li>
			<li>Probabilistic CKY Parsing</li>
		</ul>
		</li>
		<li>PCFG Training</li>
		<li>Limitations of PCFGs</li>
		<li>Treebanks</li>
		<li>Evaluating model performance</li>
		<li>Alternative Formalisms</li>
	</ul>
	</li>
	<li><span style="font-family:tahoma,sans-serif"><strong>Week 5 Linear Text Classification</strong></span>
	<ul>
		<li>Text classification</li>
		<li>Classification methods</li>
		<li>Na&iuml;ve Bayes Model</li>
		<li>Logistic Regression Model</li>
		<li>Evaluation</li>
	</ul>
	</li>
	<li><span style="font-family:tahoma,sans-serif"><strong>Week 6 Neural Networks and Neural Language Models&nbsp;</strong></span>
	<ul>
		<li>Introduction to Neural Networks</li>
		<li>The challenge of statistical language modelling</li>
		<li>Neural n-gram language models</li>
		<li>Recurrent language models</li>
		<li>A few key papers (not examinable)</li>
	</ul>
	</li>
	<li><span style="font-family:tahoma,sans-serif"><strong>Week 7 Neural Machine Translation</strong></span>
	<ul>
		<li><span style="font-family:tahoma,sans-serif">Machine Translation</span></li>
		<li><span style="font-family:tahoma,sans-serif">Decoding Algorithms</span></li>
		<li><span style="font-family:tahoma,sans-serif">Sequence-to-Sequence Models</span></li>
		<li><span style="font-family:tahoma,sans-serif">Attention Mechanism</span></li>
		<li><span style="font-family:tahoma,sans-serif">Evaluation of MT systems</span></li>
		<li><span style="font-family:tahoma,sans-serif">Examples of other Encoder-Decoder tasks</span></li>
	</ul>
	</li>
	<li><span style="font-family:tahoma,sans-serif"><strong>Week 8 Distributional Semantics</strong></span>
	<ul>
		<li>Meaning and Lexical Semantics</li>
		<li>Vector Semantics</li>
		<li>Count-based Distributed Representations (tf-id, PMI)</li>
		<li>Sparse vs. Dense Representation (SVD/PCA)</li>
		<li>Word Embeddings (Word2Vec, GloVe, fastText)</li>
		<li>Contextualized Word Embeddings (ELMo, BERT)</li>
	</ul>
	</li>
	<li><span style="font-family:tahoma,sans-serif"><strong>Week 9 Transformers and Pretrained Models</strong></span>
	<ul>
		<li><span style="font-family:tahoma,sans-serif">Transformers (in details)</span></li>
		<li><span style="font-family:tahoma,sans-serif">Pretrained Large Language Models</span>
		<ul>
			<li><span style="font-family:tahoma,sans-serif">Encoders</span></li>
			<li><span style="font-family:tahoma,sans-serif">Decoders</span></li>
			<li><span style="font-family:tahoma,sans-serif">Encoder-Decoder</span></li>
		</ul>
		</li>
		<li><span style="font-family:tahoma,sans-serif">Parameter-efficient Finetuning Methods</span>
		<ul>
			<li><span style="font-family:tahoma,sans-serif">Adapters</span></li>
			<li><span style="font-family:tahoma,sans-serif">Prefix-Tuning</span></li>
			<li><span style="font-family:tahoma,sans-serif">LoRA</span></li>
		</ul>
		</li>
		<li><span style="font-family:tahoma,sans-serif">Limitations, ethics, biases, environment</span></li>
	</ul>
	</li>
	<li><span style="font-family:tahoma,sans-serif"><strong>Week 10 Neural Speech Recognition and Translation</strong></span>
	<ul>
		<li>Speech Processing Tasks</li>
		<li><span style="font-family:tahoma,sans-serif">Automatic Speech Recognition</span>
		<ul>
			<li><span style="font-family:tahoma,sans-serif">Model Design</span></li>
			<li><span style="font-family:tahoma,sans-serif">Evaluation Metrics</span></li>
		</ul>
		</li>
		<li><span style="font-family:tahoma,sans-serif">Speech Translation</span>
		<ul>
			<li><span style="font-family:tahoma,sans-serif">Model Design</span></li>
			<li><span style="font-family:tahoma,sans-serif">Evaluation Metrics</span></li>
		</ul>
		</li>
		<li><span style="font-family:tahoma,sans-serif">Pre-trained Speech Transformers</span>
		<ul>
			<li><span style="font-family:tahoma,sans-serif">Wav2vec 2</span></li>
			<li><span style="font-family:tahoma,sans-serif">Whisper Speech Encoder</span></li>
		</ul>
		</li>
		<li><span style="font-family:tahoma,sans-serif">SUPERB Evaluation Benchmark</span></li>
	</ul>
	</li>
	<li><span style="font-family:tahoma,sans-serif"><strong>Week 11 Advanced Topics in Large Language Models (I)</strong></span>
	<ul>
		<li><span style="font-family:tahoma,sans-serif">Prompting</span></li>
		<li><span style="font-family:tahoma,sans-serif">In-context zero- or few-shot prompting</span>
		<ul>
			<li><span style="font-family:tahoma,sans-serif">Emergent in-context abilities</span></li>
			<li><span style="font-family:tahoma,sans-serif">Chain-of-thought Prompting</span></li>
		</ul>
		</li>
		<li><span style="font-family:tahoma,sans-serif">Instruction-Tuning Large Language Models</span></li>
		<li><span style="font-family:tahoma,sans-serif">Alignment with Human Feedback (PPO and DPO)</span>
		<ul>
			<li><span style="font-family:tahoma,sans-serif">Policy Gradient</span></li>
			<li><span style="font-family:tahoma,sans-serif">Reward Model to Rank</span></li>
		</ul>
		</li>
		<li><span style="font-family:tahoma,sans-serif">System 1 and System 2 Modes of Reasoning</span></li>
		<li><span style="font-family:tahoma,sans-serif">Inference-Time Scaling</span></li>
		<li><span style="font-family:tahoma,sans-serif">Reasoning LLMs (GRPO, rule-based reward, distillation)</span></li>
	</ul>
	</li>
	<li><span style="font-family:tahoma,sans-serif"><strong>Week 12  Advanced Topics in Large Language Models (II)</strong></span>
	<ul>
		<li><span style="font-family:tahoma,sans-serif">LLM Augmentations</span>
		<ul>
			<li><span style="font-family:tahoma,sans-serif">Retrieval Augmented Generation (RAG)</span></li>
			<li><span style="font-family:tahoma,sans-serif">Tool-Augmentation</span></li>
			<li><span style="font-family:tahoma,sans-serif">Self-Correction</span></li>
		</ul>
		</li>
		<li><span style="font-family:tahoma,sans-serif">Language Agents</span>
		<ul>
			<li><span style="font-family:tahoma,sans-serif">Training-free: ReAct, Reflexion, Critic, LATS</span></li>
			<li><span style="font-family:tahoma,sans-serif">Fine-tuned: FireAct, TORA, Eurus</span></li>
		</ul>
		</li>
		<li><span style="font-family:tahoma,sans-serif">Agentic Workflow</span></li>
		<li><span style="font-family:tahoma,sans-serif">Output and Process Reward Models and Verifiers</span></li>
	</ul>
	</li>
</ul>
<p><span class="text-muted"><em>CE/Lecturer, Data Analysis for Semi-structured Data (FIT5212, 2022-2025). Syllabus:</em>
<ul>
	<li><strong>Week 1 Intro to semi-structured data</strong></li>
	<li><strong>Week 2 Text representation</strong>
	<ul>
		<li>Word and Document Representations (Count Vector, TF-IDF, LSA)</li>
		<li>Basic Embedding Models (Word2vec, GLoVe)</li>
		<li>Contextualized Embedding Models (ELMo, BERT)</li>
	</ul>
	</li>
	<li><strong>Week 3 Text classification</strong>
	<ul>
		<li>Data Preparation</li>
		<li>Basic Classifier&nbsp;
		<ul>
			<li>Logistic Regression Model and Cross-Entropy Loss Function</li>
		</ul>
		</li>
		<li>Neural Network-based Classifier</li>
		<li>BERT-style Classification</li>
	</ul>
	</li>
	<li><strong>Week 4 Text clustering and matrix factorisation</strong>
	<ul>
		<li>Clustering Examples</li>
		<li>Mixture Models</li>
		<li>Topic Models (LDA)</li>
		<li>Matrix Factorisation</li>
	</ul>
	</li>
	<li><strong>Week 5 Text generation</strong>
	<ul>
		<li>​​​​​​​Neural Language Models</li>
		<li>Sequence-to-Sequence models</li>
		<li>Neural Machine Translation</li>
		<li>Attention mechanism</li>
		<li>Other language generation tasks</li>
		<li>Evaluation</li>
	</ul>
	</li>
	<li><strong>Week 6 Pretrained language models</strong>
	<ul>
		<li>​​​​​​​Introduction to Transformers</li>
		<li>Pretrained Language Models
		<ul>
			<li>Pre-trained Encoder-Decoders (T5 and BART)</li>
			<li>Pre-trained Encoders (BERT family)</li>
			<li>Pre-trained Decoders (GPT family)</li>
		</ul>
		</li>
		<li>Finetuning Pretrained Language Models</li>
	</ul>
	</li>
	<li><strong>Week 7 Recommender systems (1)&nbsp;</strong>
	<ul>
		<li>​​​​​​​Introduction to recommender systems</li>
		<li>Various Recommender Systems in Real-Life Applications</li>
		<li>Content Based Recommendation Systems</li>
		<li>Collaborative Filtering Algorithms</li>
	</ul>
	</li>
	<li><strong>Week 8 Recommender systems (2)</strong>
	<ul>
		<li>​​​​​​​Matrix Factorisation for Recommender Systems</li>
		<li>Neural Collaborative Filtering</li>
		<li>Neural Matrix Factorisation</li>
	</ul>
	</li>
	<li><strong>Week 9 Graph clustering&nbsp;</strong>
	<ul>
		<li>​​​​​​​Graph Basics</li>
		<li>K-means Graph Clustering</li>
		<li>Spectral Clustering</li>
		<li>Graph Clustering Evaluation</li>
	</ul>
	</li>
	<li><strong>Week 10 Graph representation learning</strong>
	<ul>
		<li>​​​​​​​Network Embedding Principle</li>
		<li>Random Walk Based Approaches</li>
		<li>Matrix Factorization Approaches</li>
		<li>Evaluation</li>
	</ul>
	</li>
	<li><strong>Week 11 Graph Neural Networks</strong>
	<ul>
		<li>​​​​​​​Disadvantages of shallow embedding for graph data</li>
		<li>Connections of graph neural networks to traditional deep learning</li>
		<li>Basic idea of graph neural networks</li>
		<li>Graph Convolutional Network (GCN)</li>
		<li>GraphSage Network (GraphSage)</li>
		<li>Graph Attention Network (GAT)</li>
		<li>Application of graph neural networks</li>
	</ul>
	</li>
	<li><strong>Week 12 Knowledge Graphs and Text</strong>
	<ul>
		<li>​​​​​​​Knowledge Graph construction</li>
		<li>Examples of Knowledge Graphs</li>
		<li>Knowledge-intensive tasks</li>
		<li>Infusing external knowledge into text models</li>
	</ul>
	</li>
</ul>
</span></p>
	    <p><strong><a class="quiet p1" href="https://www.ucl.ac.uk/">UCL</a>.</strong><br>
			<span class="text-muted">
				<em>Co-lecturer, Applied Machine Learning II (ELEC0135, 2020)</em>.<br>
	                        <em>Module Lead, Emerging Topics in Integrated Machine Learning Systems (ELEC0139, 2020)</em>.</span></p>
<p><strong><a class="quiet p1" href="https://www.cam.ac.uk/">University of Cambridge</a>.</strong><br>
			<span class="text-muted">
				<em>Lecturer, Quantitative Methods in Analyzing Linguistic Data (QMALD, 2019)</em>.<br>
				<em>Guest Lecturer, Computational Linguistics (LI18, 2019)</em>.</span></p>
	<!--div id="contact" class="page-header">
		<h1>Contact</h1>
	</div>
		<div class="well">
			<strong>Email</strong><br/>
			<a id="myemaddr" rel="nofollow" target="_top"></a>
		</div>
	</div>

	<script src="index.js"></script-->
	<!-- Bootstrap core JavaScript; placed at the end of the document so the pages load faster -->
    <script src="styles/jquery.js"></script>
    <script>window.jQuery || document.write('<script src="bootstrap/assets/js/vendor/jquery.min.js"><\/script>')</script>
    <script src="styles/bootstrap.js"></script>
    <script src="styles/ie10-viewport-bug-workaround.js"></script>



</div></body>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-115849498-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'UA-115849498-1');
</script>
</html>
